{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41a6949",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee001e63",
   "metadata": {},
   "source": [
    "Dataset used in this homework is my project data - scraped from Glassdoor from companies that have had or is having a female CEO. This data includes 2 text components - positive and negative comments towards the company. I also have a column telling the employee's employement status. I wanted to use this dataset to build a model to predict future employee turnover based on employee comments. \n",
    "\n",
    "I tried to run this on my full data - it took forever. So I drew 1000 entries randomly for this homework. I made sure that there's no class imbalance for my outcome variable. \n",
    "\n",
    "Link to data & Code: https://pennstateoffice365-my.sharepoint.com/:f:/r/personal/tzz5177_psu_edu/Documents/597hm4?csf=1&web=1&e=Q3efK2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbf1b1",
   "metadata": {},
   "source": [
    "# Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7a447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c358d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tiany\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1363d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.read_csv(\"df_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35b3b659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       0\n",
       "       ..\n",
       "1995    1\n",
       "1996    0\n",
       "1997    0\n",
       "1998    1\n",
       "1999    0\n",
       "Name: status, Length: 2000, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all['status'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49945956",
   "metadata": {},
   "source": [
    "# version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "550e788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.5225\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.6597 - accuracy: 0.6562\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.6245 - accuracy: 0.7188\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5836 - accuracy: 0.7619\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5418 - accuracy: 0.7919\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5027 - accuracy: 0.8094\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.4680 - accuracy: 0.8219\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.8381\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.4071 - accuracy: 0.8512\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.3832 - accuracy: 0.8669\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.7335 - accuracy: 0.5850\n",
      "Test Accuracy: 0.5849999785423279\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all['review_pros'], all['status'], test_size=0.2)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a8875",
   "metadata": {},
   "source": [
    "First run with very simple settings, accuracy is .58 which is not much better than guessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "003a5a4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 1ms/step\n",
      "AUC-ROC: 0.5923308270676693\n",
      "Specificity: 0.5631578947368421\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = (y_pred > 0.5).astype('int32')\n",
    "\n",
    "# Calculate AUC-ROC\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"AUC-ROC: {auc_roc}\")\n",
    "\n",
    "# Calculate Specificity\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_class).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Specificity: {specificity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a15bb0",
   "metadata": {},
   "source": [
    "AUC-ROC = .59 and Specificity = .56. Not great. I am going to adjust learning rate to be slower, make batch size smaller, and increases epoc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58135d3c",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c805dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "100/100 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5150\n",
      "Epoch 2/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6788 - accuracy: 0.5656\n",
      "Epoch 3/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6519 - accuracy: 0.6144\n",
      "Epoch 4/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.6944\n",
      "Epoch 5/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7681\n",
      "Epoch 6/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8081\n",
      "Epoch 7/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8600\n",
      "Epoch 8/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2936 - accuracy: 0.8712\n",
      "Epoch 9/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2301 - accuracy: 0.9100\n",
      "Epoch 10/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.2003 - accuracy: 0.9169\n",
      "Epoch 11/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.1692 - accuracy: 0.9294\n",
      "Epoch 12/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.1438 - accuracy: 0.9394\n",
      "Epoch 13/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.1284 - accuracy: 0.9494\n",
      "Epoch 14/25\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1161 - accuracy: 0.9538\n",
      "Epoch 15/25\n",
      "100/100 [==============================] - 0s 5ms/step - loss: 0.0953 - accuracy: 0.9656\n",
      "Epoch 16/25\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0874 - accuracy: 0.9663\n",
      "Epoch 17/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0715 - accuracy: 0.9719\n",
      "Epoch 18/25\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9744\n",
      "Epoch 19/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0784 - accuracy: 0.9712\n",
      "Epoch 20/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0761 - accuracy: 0.9706\n",
      "Epoch 21/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9731\n",
      "Epoch 22/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9775\n",
      "Epoch 23/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0526 - accuracy: 0.9787\n",
      "Epoch 24/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0713 - accuracy: 0.9762\n",
      "Epoch 25/25\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0665 - accuracy: 0.9781\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "AUC-ROC: 0.6074891970656215\n",
      "Specificity: 0.4719626168224299\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.4705 - accuracy: 0.5875\n",
      "Test Loss: 2.47048282623291\n",
      "Test Accuracy: 0.5874999761581421\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "model_v2 = Sequential()\n",
    "model_v2.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model_v2.add(Dropout(0.5))\n",
    "model_v2.add(Dense(32, activation='relu'))\n",
    "model_v2.add(Dropout(0.5))\n",
    "model_v2.add(Dense(16, activation='relu'))\n",
    "model_v2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model_v2.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_v2.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "\n",
    "y_pred_prob_v2 = model_v2.predict(X_test)\n",
    "y_pred_class_v2 = (y_pred_prob_v2 > 0.5).astype('int32')\n",
    "\n",
    "\n",
    "auc_roc_v2 = roc_auc_score(y_test, y_pred_prob_v2)\n",
    "print(f\"AUC-ROC: {auc_roc_v2}\")\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_class_v2).ravel()\n",
    "specificity_v2 = tn / (tn + fp)\n",
    "print(f\"Specificity: {specificity_v2}\")\n",
    "\n",
    "loss_v2, accuracy_v2 = model_v2.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss_v2}\")\n",
    "print(f\"Test Accuracy: {accuracy_v2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17e6ba",
   "metadata": {},
   "source": [
    "AUC-ROC = .61, Specificity = .47, Accuracy = .59. I am going to try to use CNN in the next model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6473de87",
   "metadata": {},
   "source": [
    "# Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ddf9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - 1s 6ms/step - loss: 0.6957 - accuracy: 0.4831\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.6805 - accuracy: 0.5894\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.6444 - accuracy: 0.6869\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.5807 - accuracy: 0.7437\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.4640 - accuracy: 0.8181\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.3342 - accuracy: 0.8850\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.2565 - accuracy: 0.9175\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.1901 - accuracy: 0.9362\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.1509 - accuracy: 0.9475\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.1281 - accuracy: 0.9613\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1078 - accuracy: 0.9600\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0868 - accuracy: 0.9669\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0816 - accuracy: 0.9688\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0732 - accuracy: 0.9688\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0629 - accuracy: 0.9787\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0603 - accuracy: 0.9712\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0553 - accuracy: 0.9725\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0485 - accuracy: 0.9800\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0482 - accuracy: 0.9737\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0587 - accuracy: 0.9700\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0448 - accuracy: 0.9812\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0488 - accuracy: 0.9750\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0431 - accuracy: 0.9831\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0446 - accuracy: 0.9775\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0391 - accuracy: 0.9787\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0429 - accuracy: 0.9750\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0400 - accuracy: 0.9762\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0409 - accuracy: 0.9794\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0324 - accuracy: 0.9806\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 0.0410 - accuracy: 0.9756\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "AUC-ROC: 0.6158599124452783\n",
      "Specificity: 0.6102564102564103\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.3169 - accuracy: 0.5700\n",
      "Test Loss: 2.316941022872925\n",
      "Test Accuracy: 0.5699999928474426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_features_v3 = 10000  # Size of the vocabulary\n",
    "maxlen_v3 = 100         # Maximum length of each sequence\n",
    "tokenizer_v3 = Tokenizer(num_words=max_features_v3)\n",
    "tokenizer_v3.fit_on_texts(all['review_pros'])\n",
    "X_v3 = tokenizer_v3.texts_to_sequences(all['review_pros'])\n",
    "X_v3 = pad_sequences(X_v3, maxlen=maxlen_v3)\n",
    "X_train_v3, X_test_v3, y_train_v3, y_test_v3 = train_test_split(X_v3, all['status'], test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN model architecture\n",
    "model_v3 = Sequential()\n",
    "model_v3.add(Embedding(max_features_v3, 50, input_length=maxlen_v3))\n",
    "model_v3.add(Conv1D(64, 5, activation='relu'))\n",
    "model_v3.add(GlobalMaxPooling1D())\n",
    "model_v3.add(Dense(10, activation='relu'))\n",
    "model_v3.add(Dropout(0.5))\n",
    "model_v3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimizer_v3 = Adam(learning_rate=0.001)\n",
    "model_v3.compile(optimizer=optimizer_v3, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_v3.fit(X_train_v3, y_train_v3, epochs=30, batch_size=32)\n",
    "\n",
    "\n",
    "y_pred_prob_v3 = model_v3.predict(X_test_v3)\n",
    "y_pred_class_v3 = (y_pred_prob_v3 > 0.5).astype('int32')\n",
    "\n",
    "\n",
    "auc_roc_v3 = roc_auc_score(y_test_v3, y_pred_prob_v3)\n",
    "print(f\"AUC-ROC: {auc_roc_v3}\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_v3, y_pred_class_v3).ravel()\n",
    "specificity_v3 = tn / (tn + fp)\n",
    "print(f\"Specificity: {specificity_v3}\")\n",
    "loss_v3, accuracy_v3 = model_v3.evaluate(X_test_v3, y_test_v3)\n",
    "print(f\"Test Loss: {loss_v3}\")\n",
    "print(f\"Test Accuracy: {accuracy_v3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb8a2b",
   "metadata": {},
   "source": [
    "AUC-ROC = .62 and Specificity = .61. This version is better than v2. I want to start using pre-trained embedding in the next version. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5d16b",
   "metadata": {},
   "source": [
    "# Version 4 but I accidently named it v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8898dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download and load the Word2Vec model from Gensim\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "172f16e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - 1s 7ms/step - loss: 0.6954 - accuracy: 0.5169\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.6763 - accuracy: 0.5763\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.6512 - accuracy: 0.6169\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.6328 - accuracy: 0.6294\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.5792 - accuracy: 0.7081\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.5160 - accuracy: 0.7569\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.4375 - accuracy: 0.8112\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.3548 - accuracy: 0.8600\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.3000 - accuracy: 0.9000\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2471 - accuracy: 0.9281\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1919 - accuracy: 0.9544\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1611 - accuracy: 0.9594\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1412 - accuracy: 0.9606\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1253 - accuracy: 0.9694\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1067 - accuracy: 0.9744\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1022 - accuracy: 0.9750\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1071 - accuracy: 0.9719\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0899 - accuracy: 0.9787\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.0952 - accuracy: 0.9825\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.0840 - accuracy: 0.9775\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0860 - accuracy: 0.9850\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0823 - accuracy: 0.9769\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0729 - accuracy: 0.9819\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0663 - accuracy: 0.9825\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0714 - accuracy: 0.9837\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0764 - accuracy: 0.9781\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0587 - accuracy: 0.9825\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0678 - accuracy: 0.9837\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0639 - accuracy: 0.9831\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0707 - accuracy: 0.9837\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.6961 - accuracy: 0.5625\n",
      "Test Loss: 1.696050763130188\n",
      "Test Accuracy: 0.5625\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "AUC-ROC: 0.5497185741088181\n",
      "Specificity: 0.358974358974359\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "max_features_v5 = 10000\n",
    "maxlen_v5 = 100\n",
    "tokenizer_v5 = Tokenizer(num_words=max_features_v5)\n",
    "tokenizer_v5.fit_on_texts(all['review_pros'])\n",
    "X_v5 = tokenizer_v5.texts_to_sequences(all['review_pros'])\n",
    "X_v5 = pad_sequences(X_v5, maxlen=maxlen_v5)\n",
    "X_train_v5, X_test_v5, y_train_v5, y_test_v5 = train_test_split(X_v5, all['status'], test_size=0.2, random_state=42)\n",
    "\n",
    "embedding_dim_v5 = 300\n",
    "embedding_matrix_v5 = np.zeros((max_features_v5, embedding_dim_v5))\n",
    "for word, i in tokenizer_v5.word_index.items():\n",
    "    if i < max_features_v5 and word in word_vectors.key_to_index:\n",
    "        embedding_matrix_v5[i] = word_vectors[word]\n",
    "\n",
    "model_v5 = Sequential()\n",
    "model_v5.add(Embedding(max_features_v5, embedding_dim_v5, input_length=maxlen_v5,\n",
    "                       weights=[embedding_matrix_v5], trainable=False))\n",
    "model_v5.add(Conv1D(64, 5, activation='relu'))\n",
    "model_v5.add(GlobalMaxPooling1D())\n",
    "model_v5.add(Dense(10, activation='relu'))\n",
    "model_v5.add(Dropout(0.5))\n",
    "model_v5.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model_v5.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_v5.fit(X_train_v5, y_train_v5, epochs=30, batch_size=32)\n",
    "\n",
    "\n",
    "loss_v5, accuracy_v5 = model_v5.evaluate(X_test_v5, y_test_v5)\n",
    "print(f\"Test Loss: {loss_v5}\")\n",
    "print(f\"Test Accuracy: {accuracy_v5}\")\n",
    "\n",
    "\n",
    "y_pred_prob_v5 = model_v5.predict(X_test_v5)\n",
    "y_pred_class_v5 = (y_pred_prob_v5 > 0.5).astype('int32')\n",
    "auc_roc_v5 = roc_auc_score(y_test_v5, y_pred_prob_v5)\n",
    "print(f\"AUC-ROC: {auc_roc_v5}\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_v5, y_pred_class_v5).ravel()\n",
    "specificity_v5 = tn / (tn + fp)\n",
    "print(f\"Specificity: {specificity_v5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd10fbd",
   "metadata": {},
   "source": [
    "Using Word2Vec actually makes my model perform worse - could be that the training set of it does not fit my current data (news vs. online comment). I am going to make one last try by modifying the complexity of my neural network and adjusting layer size and number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e95dc",
   "metadata": {},
   "source": [
    "# Version 5 (v6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "956f20f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - 1s 7ms/step - loss: 0.6940 - accuracy: 0.5050\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.6875 - accuracy: 0.5675\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.6679 - accuracy: 0.6438\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.6090 - accuracy: 0.7356\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.4949 - accuracy: 0.7944\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.3741 - accuracy: 0.8656\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.2564 - accuracy: 0.9194\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1889 - accuracy: 0.9450\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.1406 - accuracy: 0.9631\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.1160 - accuracy: 0.9719\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0916 - accuracy: 0.9800\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0810 - accuracy: 0.9787\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0631 - accuracy: 0.9862\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0578 - accuracy: 0.9856\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0443 - accuracy: 0.9894\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0547 - accuracy: 0.9837\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0439 - accuracy: 0.9900\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0356 - accuracy: 0.9919\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0335 - accuracy: 0.9912\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0341 - accuracy: 0.9906\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0342 - accuracy: 0.9894\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0395 - accuracy: 0.9887\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0245 - accuracy: 0.9944\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0269 - accuracy: 0.9919\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0241 - accuracy: 0.9937\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0281 - accuracy: 0.9912\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0263 - accuracy: 0.9931\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 0.0236 - accuracy: 0.9925\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0226 - accuracy: 0.9937\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.0231 - accuracy: 0.9956\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.6524 - accuracy: 0.5925\n",
      "Test Loss: 2.6523804664611816\n",
      "Test Accuracy: 0.5924999713897705\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "AUC-ROC: 0.6364727954971858\n",
      "Specificity: 0.7128205128205128\n"
     ]
    }
   ],
   "source": [
    "max_features_v6 = 10000  \n",
    "maxlen_v6 = 100          \n",
    "\n",
    "tokenizer_v6 = Tokenizer(num_words=max_features_v6)\n",
    "tokenizer_v6.fit_on_texts(all['review_pros'])\n",
    "X_v6 = tokenizer_v6.texts_to_sequences(all['review_pros'])\n",
    "X_v6 = pad_sequences(X_v6, maxlen=maxlen_v6)\n",
    "\n",
    "X_train_v6, X_test_v6, y_train_v6, y_test_v6 = train_test_split(X_v6, all['status'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Adjusted model architecture\n",
    "model_v6 = Sequential()\n",
    "model_v6.add(Embedding(max_features_v6, 50, input_length=maxlen_v6))  # Embedding layer\n",
    "model_v6.add(Conv1D(128, 3, activation='relu'))  # Increased number of filters and changed kernel size\n",
    "model_v6.add(GlobalMaxPooling1D())\n",
    "model_v6.add(Dense(20, activation='relu'))  # Increased size of this dense layer\n",
    "model_v6.add(Dropout(0.5))  # Dropout for regularization\n",
    "model_v6.add(Dense(10, activation='relu'))  # Additional dense layer\n",
    "model_v6.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "\n",
    "model_v6.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_v6.fit(X_train_v6, y_train_v6, epochs=30, batch_size=32)\n",
    "\n",
    "\n",
    "loss_v6, accuracy_v6 = model_v6.evaluate(X_test_v6, y_test_v6)\n",
    "print(f\"Test Loss: {loss_v6}\")\n",
    "print(f\"Test Accuracy: {accuracy_v6}\")\n",
    "\n",
    "\n",
    "y_pred_prob_v6 = model_v6.predict(X_test_v6)\n",
    "y_pred_class_v6 = (y_pred_prob_v6 > 0.5).astype('int32')\n",
    "auc_roc_v6 = roc_auc_score(y_test_v6, y_pred_prob_v6)\n",
    "print(f\"AUC-ROC: {auc_roc_v6}\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_v6, y_pred_class_v6).ravel()\n",
    "specificity_v6 = tn / (tn + fp)\n",
    "print(f\"Specificity: {specificity_v6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b18fc4",
   "metadata": {},
   "source": [
    "This version performs a lot better than the previous version. Accuracy = .59, AUC-ROC = .64, and Specificity (the thing I cared abotu the most) = .71. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86bc24f",
   "metadata": {},
   "source": [
    "# Version 6 (v7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8011415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - 2s 10ms/step - loss: 0.6946 - accuracy: 0.4856\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.6927 - accuracy: 0.5019\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.6931 - accuracy: 0.5163\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.6809 - accuracy: 0.5756\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.6322 - accuracy: 0.6431\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.5478 - accuracy: 0.7394\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.4089 - accuracy: 0.8306\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.3103 - accuracy: 0.8819\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.2453 - accuracy: 0.9106\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1832 - accuracy: 0.9281\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 1s 24ms/step - loss: 0.1318 - accuracy: 0.9556\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0943 - accuracy: 0.9638\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0921 - accuracy: 0.9638\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1106 - accuracy: 0.9581\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0812 - accuracy: 0.9712\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0585 - accuracy: 0.9775\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0450 - accuracy: 0.9819\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0394 - accuracy: 0.9819\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0449 - accuracy: 0.9875\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0371 - accuracy: 0.9837\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0613 - accuracy: 0.9769\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0482 - accuracy: 0.9800\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0606 - accuracy: 0.9787\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0408 - accuracy: 0.9869\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0410 - accuracy: 0.9869\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0247 - accuracy: 0.9900\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0207 - accuracy: 0.9931\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0237 - accuracy: 0.9887\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0206 - accuracy: 0.9887\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0195 - accuracy: 0.9906\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 6.3784 - accuracy: 0.5475\n",
      "Test Loss: 6.378418445587158\n",
      "Test Accuracy: 0.5475000143051147\n",
      "13/13 [==============================] - 0s 5ms/step\n",
      "AUC-ROC: 0.5863539712320199\n",
      "Specificity: 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "# Parameters (based on the previous successful model)\n",
    "max_features_v7 = 8000  # Keeping the same as v6\n",
    "maxlen_v7 = 100         # Keeping the same as v6\n",
    "\n",
    "# Preprocessing (same as v6)\n",
    "tokenizer_v7 = Tokenizer(num_words=max_features_v7)\n",
    "tokenizer_v7.fit_on_texts(all['review_pros'])\n",
    "X_v7 = tokenizer_v7.texts_to_sequences(all['review_pros'])\n",
    "X_v7 = pad_sequences(X_v7, maxlen=maxlen_v7)\n",
    "X_train_v7, X_test_v7, y_train_v7, y_test_v7 = train_test_split(X_v7, all['status'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Adjusted model architecture\n",
    "model_v7 = Sequential()\n",
    "model_v7.add(Embedding(max_features_v7, 50, input_length=maxlen_v7))\n",
    "model_v7.add(Conv1D(128, 3, activation='relu'))\n",
    "model_v7.add(Conv1D(128, 3, activation='relu'))  # Additional convolutional layer\n",
    "model_v7.add(GlobalMaxPooling1D())\n",
    "model_v7.add(Dense(30, activation='relu'))  # Increased neurons in this layer\n",
    "model_v7.add(Dropout(0.5))\n",
    "model_v7.add(Dense(15, activation='relu'))  # Another dense layer with more neurons\n",
    "model_v7.add(Dropout(0.5))  # Additional dropout layer for regularization\n",
    "model_v7.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model_v7.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_v7.fit(X_train_v7, y_train_v7, epochs=30, batch_size=32)\n",
    "\n",
    "\n",
    "loss_v7, accuracy_v7 = model_v7.evaluate(X_test_v7, y_test_v7)\n",
    "print(f\"Test Loss: {loss_v7}\")\n",
    "print(f\"Test Accuracy: {accuracy_v7}\")\n",
    "\n",
    "y_pred_prob_v7 = model_v7.predict(X_test_v7)\n",
    "y_pred_class_v7 = (y_pred_prob_v7 > 0.5).astype('int32')\n",
    "auc_roc_v7 = roc_auc_score(y_test_v7, y_pred_prob_v7)\n",
    "print(f\"AUC-ROC: {auc_roc_v7}\")\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_v7, y_pred_class_v7).ravel()\n",
    "specificity_v7 = tn / (tn + fp)\n",
    "print(f\"Specificity: {specificity_v7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d9f54",
   "metadata": {},
   "source": [
    "worse than v6 maybe it overfit. I am going to keep v6 as my final model for this homework. Now to test how well it works - I am going to write a few sentences myself and to use my model to predict. I tried to think of how I'd rate these companies if I worked for them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65e1cc",
   "metadata": {},
   "source": [
    "# Testing Model Performance with my Writings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0462894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample text entries\n",
    "texts = [\n",
    "    \"Honestly, I don't know what to say about this place, I really can't think of anything good to say. I HATE HATE my job at Walgreens.\",\n",
    "    \"Bed bath and beyond is honestly the best employer I have, people are nice, the benefits are good, and my manager she's like an angel. It's really the people that make your experience great.\",\n",
    "    \"Payments are good, work-life balance is good, and end of year benefit is decent.\",\n",
    "    \"It's a big company across the world so you'd get a lot of travel opportunities. I guess it could be good or bad. I like the free lunch provided there.\",\n",
    "    \"NONE None NONE DON'T work for them.\",\n",
    "    \"This job makes me want to kms.\",\n",
    "    \"They have free daycare in the building, which to me is the most amazing thing = I can leave my kid there and visit her during the day within my office building. I think this by itself makes me want to work for Oracle forever.\",\n",
    "    \"Employee discount is pretty good. Sometimes you get 40% off for some latest shoes, and you can always get the first dibs on all the popular ones.\"\n",
    "]\n",
    "\n",
    "\n",
    "X_sample = tokenizer_v6.texts_to_sequences(texts)\n",
    "X_sample = pad_sequences(X_sample, maxlen=maxlen_v6)\n",
    "predictions = model_v6.predict(X_sample)\n",
    "predicted_classes = (predictions > 0.5).astype(int)\n",
    "\n",
    "\n",
    "predicted_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb9e246",
   "metadata": {},
   "source": [
    "1. honestly I don't know what to say about this place, I really can't think of anything good to say. I HATE HATE my job at Walgreens. \n",
    "[Model prediction - Quit]\n",
    "\n",
    "2. bed bath and beyond is hoenstly the best employers I have, people are nice, the benefits are good, and my manager she's like an angel. It's really the people tha make your experience great. \n",
    "[Model prediction - Quit]\n",
    "\n",
    "3. payments are good, work-life balance is good, and end of year benefit is decent. \n",
    "[Model prediction - Stay]\n",
    "\n",
    "4. it's a big company across the world so you'd get a lot of travel opportunities. I guess it could be good or bad. I like the free lunch provided there.\n",
    "[Model prediction - Quit]\n",
    "\n",
    "5. NONe None NONE DON't work for them\n",
    "[Model prediction - Quit]\n",
    "\n",
    "6. This job makes me want to kms\n",
    "[Model prediction - Stay]\n",
    "\n",
    "7. they have free daycare in the building, which to me is the most amazing thing = I can leave my kid there and visit her during the day within my office buidling. I think this by itself makes me want to work for oracle forever. \n",
    "[Model prediction - Stay]\n",
    "\n",
    "8. employee discount is pretty good. sometimes you get 40% off for some latest shoes, and you can always get the first dibs on all the popular ones. \n",
    "[Model prediction - Quit]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a16ed0",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0c488",
   "metadata": {},
   "source": [
    "I think this model I built is decently good - it defnitely captured the most intense one: 1, 5 and 7, which is super negative and super positive. It missed 6 - does not capture internet language too well, which makes sense. The model defnitely is more prone to categorize quit than stay, which aligns with my original intention. I want this to be something to capture employees with even remote turnover potentials, so that maybe companies could target some preventions. I see that for ambiguous comments such as 4 and 8 - the model categorized them as Quit, which could go either way with human interpretation. Afterall - had I have a different purpose, like wanting to maximize other metrics, this model might not be selected. \n",
    "\n",
    "The model can defnitely be improved if the original training data was to measure turnover intention instead of their actualy employement status - since employement status can be polluted by a number of other stuff compared to turnover intention. This model only used 2000 entries of data and I am content with it's current performance. If more data can be thrown in and use a more complicated word embedding such as bert, it might work better. (I tried using bert but one epoc with only 600 entries took more than 12 hours and I had to interrupt and change strategy). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
